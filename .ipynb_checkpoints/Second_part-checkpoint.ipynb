{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1884bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab138f",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8628473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"kaggle_data/train_data.pkl\")\n",
    "test_data = pd.read_pickle(\"kaggle_data/test_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f51b2",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fe9a9",
   "metadata": {},
   "source": [
    "## Selecting all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2406f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy             516017\n",
       "anticipation    248935\n",
       "trust           205478\n",
       "sadness         193437\n",
       "disgust         139101\n",
       "fear             63999\n",
       "surprise         48729\n",
       "anger            39867\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fd9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_data = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc48e56",
   "metadata": {},
   "source": [
    "## Equalize the number of data (work bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_data = train_data.groupby(\"emotion\").sample(n=39867, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_data.sample(frac=1)\n",
    "sample_train_data['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d45f83",
   "metadata": {},
   "source": [
    "## Text vectorization use TFIDF and stemmer (work bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5210bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_datas = []\n",
    "for sentence in sample_train_data['text']:\n",
    "    stem_datas.append(stemSentence(sentence))\n",
    "\n",
    "sample_train_data['stem_text'] = stem_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_datas = []\n",
    "for sentence in test_data['text']:\n",
    "    stem_datas.append(stemSentence(sentence))\n",
    "\n",
    "test_data['stem_text'] = stem_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_vectorizer = CountVectorizer(max_features=500, tokenizer=word_tokenize)\n",
    "TFIDF_vectorizer.fit(sample_train_data['stem_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a339022",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_tokenized = TFIDF_vectorizer.transform(sample_train_data['stem_text'])\n",
    "trained_answer = TFIDF_train_data['emotion']\n",
    "target = TFIDF_vectorizer.transform(test_data['stem_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(trained_tokenized, trained_answer, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6937b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178caa5c",
   "metadata": {},
   "source": [
    "## Text vectorization with word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(sample_train_data['text'])\n",
    "\n",
    "trained_vectors = tokenizer.texts_to_sequences(sample_train_data['text'])\n",
    "target = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "print(sample_train_data['text'].iloc[2])\n",
    "print(trained_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e628fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "trained_vectors = pad_sequences(trained_vectors, padding='post', maxlen=maxlen)\n",
    "trained_answer = sample_train_data['emotion']\n",
    "target = pad_sequences(target, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(trained_vectors[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9918ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(trained_vectors, trained_answer, test_size=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a835a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341c79c",
   "metadata": {},
   "source": [
    "## with pretrained w2v model (use glove-twitter 27B 100d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb2e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('./glove/glove.twitter.27B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50418e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sample_train_data['text'])\n",
    "\n",
    "# integer encode the documents\n",
    "trained_vectors = t.texts_to_sequences(sample_train_data['text'])\n",
    "target = t.texts_to_sequences(test_data['text'])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# padding\n",
    "maxlen = 100\n",
    "trained_vectors = pad_sequences(trained_vectors, maxlen=maxlen, padding='post')\n",
    "trained_answer = sample_train_data['emotion']\n",
    "target = pad_sequences(target, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c310d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建造可以轉換為GloVe 100維 詞向量的矩陣\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edfed61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(trained_vectors, trained_answer, test_size=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef671634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1411896, 100)\n",
      "(1411896,)\n",
      "(43667, 100)\n",
      "(43667,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abef86",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fba9d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "## Before conver\n",
      "\n",
      "78570         joy\n",
      "847092        joy\n",
      "700362      anger\n",
      "78141     sadness\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "\n",
      "## After convert\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_data['emotion'])\n",
    "print(\"classes:\", label_encoder.classes_)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "print('## Before conver\\n')\n",
    "print(Y_train[0:4])\n",
    "def label_encode(le, oe, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return oe.fit_transform(enc.reshape((len(enc), 1)))\n",
    "    #return enc\n",
    "    \n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "Y_train = label_encode(label_encoder, onehot_encoder, Y_train)\n",
    "Y_test = label_encode(label_encoder, onehot_encoder, Y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print(Y_train[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948945cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  100\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e911e84",
   "metadata": {},
   "source": [
    "# Predict (deep learning is better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78de4d",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47be464",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "DT_model = DT_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_test_pred = DT_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy\n",
    "acc_train = accuracy_score(y_true=Y_test, y_pred=Y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0833391",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=Y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb625f0b",
   "metadata": {},
   "source": [
    "### naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_test_pred = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b52397",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_true=Y_test, y_pred=Y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600664bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=Y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83a31b",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ab423",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(n_estimators=100,n_jobs = -1,random_state =50, min_samples_leaf = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = forest_model.fit(X_train, Y_train)\n",
    "Y_test_pred = forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a28b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_true=Y_test, y_pred=Y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1605c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=Y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ebfa3",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbdeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LR.fit(X_train, Y_train)\n",
    "Y_test_pred = LR.predict(X_test)\n",
    "target_result = LR.predict(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_true=Y_test, y_pred=Y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=Y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46f57a",
   "metadata": {},
   "source": [
    "## keras with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf348122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 01:46:59.250333: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f2c9c3343d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae164a75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          91266200  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 80)           45120     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 80)           38720     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 80)                38720     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 648       \n",
      "=================================================================\n",
      "Total params: 91,389,408\n",
      "Trainable params: 123,208\n",
      "Non-trainable params: 91,266,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# without w2v model\n",
    "model.add(layers.Embedding(input_dim=10000, \n",
    "                           output_dim=50, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.Bidirectional(layers.LSTM(40, return_sequences=True)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(40, return_sequences=True)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(40)))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "\n",
    "'''\n",
    "# embedding layer with pretrained w2v model\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=100, \n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=maxlen,\n",
    "                           trainable=False))\n",
    "model.add(layers.Bidirectional(layers.LSTM(40, return_sequences=True)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(40, return_sequences=True)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(40)))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "'''\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a12833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.2375 - accuracy: 0.5496 - val_loss: 1.1883 - val_accuracy: 0.5676\n",
      "Epoch 2/20\n",
      "141190/141190 [==============================] - 1834s 13ms/step - loss: 1.1560 - accuracy: 0.5802 - val_loss: 1.1641 - val_accuracy: 0.5783\n",
      "Epoch 3/20\n",
      "141190/141190 [==============================] - 1818s 13ms/step - loss: 1.1338 - accuracy: 0.5887 - val_loss: 1.1558 - val_accuracy: 0.5811\n",
      "Epoch 4/20\n",
      "141190/141190 [==============================] - 1814s 13ms/step - loss: 1.1213 - accuracy: 0.5928 - val_loss: 1.1504 - val_accuracy: 0.5833\n",
      "Epoch 5/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.1132 - accuracy: 0.5963 - val_loss: 1.1528 - val_accuracy: 0.5830\n",
      "Epoch 6/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.1072 - accuracy: 0.5982 - val_loss: 1.1494 - val_accuracy: 0.5840\n",
      "Epoch 7/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.1022 - accuracy: 0.6000 - val_loss: 1.1460 - val_accuracy: 0.5836\n",
      "Epoch 8/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0985 - accuracy: 0.6015 - val_loss: 1.1434 - val_accuracy: 0.5843\n",
      "Epoch 9/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0953 - accuracy: 0.6030 - val_loss: 1.1480 - val_accuracy: 0.5836\n",
      "Epoch 10/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0936 - accuracy: 0.6034 - val_loss: 1.1431 - val_accuracy: 0.5856\n",
      "Epoch 11/20\n",
      "141190/141190 [==============================] - 1816s 13ms/step - loss: 1.0913 - accuracy: 0.6040 - val_loss: 1.1420 - val_accuracy: 0.5881\n",
      "Epoch 12/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0895 - accuracy: 0.6050 - val_loss: 1.1421 - val_accuracy: 0.5866\n",
      "Epoch 13/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0865 - accuracy: 0.6062 - val_loss: 1.1473 - val_accuracy: 0.5861\n",
      "Epoch 14/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0872 - accuracy: 0.6058 - val_loss: 1.1477 - val_accuracy: 0.5837\n",
      "Epoch 15/20\n",
      "141190/141190 [==============================] - 1814s 13ms/step - loss: 1.0837 - accuracy: 0.6071 - val_loss: 1.1453 - val_accuracy: 0.5855\n",
      "Epoch 16/20\n",
      "141190/141190 [==============================] - 1814s 13ms/step - loss: 1.0844 - accuracy: 0.6070 - val_loss: 1.1429 - val_accuracy: 0.5863\n",
      "Epoch 17/20\n",
      "141190/141190 [==============================] - 1814s 13ms/step - loss: 1.0831 - accuracy: 0.6077 - val_loss: 1.1467 - val_accuracy: 0.5854\n",
      "Epoch 18/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0823 - accuracy: 0.6076 - val_loss: 1.1432 - val_accuracy: 0.5865\n",
      "Epoch 19/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0810 - accuracy: 0.6079 - val_loss: 1.1495 - val_accuracy: 0.5839\n",
      "Epoch 20/20\n",
      "141190/141190 [==============================] - 1815s 13ms/step - loss: 1.0803 - accuracy: 0.6083 - val_loss: 1.1445 - val_accuracy: 0.5852\n",
      "44122/44122 [==============================] - 252s 6ms/step - loss: 1.0630 - accuracy: 0.6147\n",
      "Training Accuracy: 0.6147\n",
      "1365/1365 [==============================] - 8s 6ms/step - loss: 1.1445 - accuracy: 0.5852\n",
      "Testing Accuracy:  0.5852\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, Y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf1412ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f2a702eac10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: [Errno 28] No space left on device\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f2a702eac10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: [Errno 28] No space left on device\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'anticipation', 'joy', 'joy', 'trust'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_result = model.predict(target)\n",
    "target_result = label_decode(label_encoder, target_result)\n",
    "target_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289297d1",
   "metadata": {},
   "source": [
    "# result to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c1ec684",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv = pd.DataFrame(columns=['id', 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5697f0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hashtags  tweet_id  \\\n",
       "2                             [bibleverse]  0x28b412   \n",
       "4                                       []  0x2de201   \n",
       "9        [materialism, money, possessions]  0x218443   \n",
       "30                    [GodsPlan, GodsWork]  0x2939d5   \n",
       "33                                      []  0x26289a   \n",
       "...                                    ...       ...   \n",
       "1867525                                 []  0x2913b4   \n",
       "1867529                                 []  0x2a980e   \n",
       "1867530    [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "1867531                                 []  0x29d0cb   \n",
       "1867532                                 []  0x2a6a4f   \n",
       "\n",
       "                                                      text identification  \n",
       "2        Confident of your obedience, I write to you, k...           test  \n",
       "4        \"Trust is not the same as faith. A friend is s...           test  \n",
       "9        When do you have enough ? When are you satisfi...           test  \n",
       "30       God woke you up, now chase the day #GodsPlan #...           test  \n",
       "33       In these tough times, who do YOU turn to as yo...           test  \n",
       "...                                                    ...            ...  \n",
       "1867525  \"For this is the message that ye heard from th...           test  \n",
       "1867529  \"There is a lad here, which hath five barley l...           test  \n",
       "1867530  When you buy the last 2 tickets remaining for ...           test  \n",
       "1867531  I swear all this hard work gone pay off one da...           test  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...           test  \n",
       "\n",
       "[411972 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d71e503b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id emotion\n",
       "2        0x28b412     NaN\n",
       "4        0x2de201     NaN\n",
       "9        0x218443     NaN\n",
       "30       0x2939d5     NaN\n",
       "33       0x26289a     NaN\n",
       "...           ...     ...\n",
       "1867525  0x2913b4     NaN\n",
       "1867529  0x2a980e     NaN\n",
       "1867530  0x316b80     NaN\n",
       "1867531  0x29d0cb     NaN\n",
       "1867532  0x2a6a4f     NaN\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_csv['id'] = test_data['tweet_id']\n",
    "result_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac2f5dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       emotion\n",
       "2        0x28b412  anticipation\n",
       "4        0x2de201  anticipation\n",
       "9        0x218443           joy\n",
       "30       0x2939d5           joy\n",
       "33       0x26289a         trust\n",
       "...           ...           ...\n",
       "1867525  0x2913b4  anticipation\n",
       "1867529  0x2a980e  anticipation\n",
       "1867530  0x316b80       sadness\n",
       "1867531  0x29d0cb           joy\n",
       "1867532  0x2a6a4f       sadness\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_csv['emotion'] = target_result\n",
    "result_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f6ee5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv.to_csv(\"kaggle_data/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0dd57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
