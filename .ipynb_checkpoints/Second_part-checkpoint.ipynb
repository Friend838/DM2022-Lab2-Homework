{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1884bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab138f",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8628473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"kaggle_data/train_data.pkl\")\n",
    "test_data = pd.read_pickle(\"kaggle_data/test_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f51b2",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fe9a9",
   "metadata": {},
   "source": [
    "## Selecting n samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2406f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy             516017\n",
       "anticipation    248935\n",
       "trust           205478\n",
       "sadness         193437\n",
       "disgust         139101\n",
       "fear             63999\n",
       "surprise         48729\n",
       "anger            39867\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fd9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d1fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_data = train_data.groupby(\"emotion\").sample(n=39867, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f3bace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disgust         39867\n",
       "surprise        39867\n",
       "joy             39867\n",
       "trust           39867\n",
       "anger           39867\n",
       "fear            39867\n",
       "sadness         39867\n",
       "anticipation    39867\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_data.sample(frac=1)\n",
    "sample_train_data['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d45f83",
   "metadata": {},
   "source": [
    "## Text vectorization use TFIDF and stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5210bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a6c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_datas = []\n",
    "for sentence in sample_train_data['text']:\n",
    "    stem_datas.append(stemSentence(sentence))\n",
    "\n",
    "sample_train_data['stem_text'] = stem_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74d5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_datas = []\n",
    "for sentence in test_data['text']:\n",
    "    stem_datas.append(stemSentence(sentence))\n",
    "\n",
    "test_data['stem_text'] = stem_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce5d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=500,\n",
       "                tokenizer=<function word_tokenize at 0x000002CD80ED6820>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_vectorizer = CountVectorizer(max_features=500, tokenizer=word_tokenize)\n",
    "TFIDF_vectorizer.fit(sample_train_data['stem_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a339022",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_tokenized = TFIDF_vectorizer.transform(sample_train_data['stem_text'])\n",
    "trained_answer = TFIDF_train_data['emotion']\n",
    "target = TFIDF_vectorizer.transform(test_data['stem_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8e73a",
   "metadata": {},
   "source": [
    "## Split train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c896e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(trained_tokenized, trained_answer, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6937b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 500)\n",
      "(180000,)\n",
      "(60000, 500)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178caa5c",
   "metadata": {},
   "source": [
    "## Text vectorization with word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3744b6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290704\n",
      "@BakaShift really Alain? You just followed me now? <LH>\n",
      "[89, 6, 25, 1662, 17, 51, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(sample_train_data['text'])\n",
    "\n",
    "trained_vectors = tokenizer.texts_to_sequences(sample_train_data['text'])\n",
    "target = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "print(sample_train_data['text'].iloc[2])\n",
    "print(trained_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e628fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3   81   53    6  811   22   30    4   36   95    1 2916    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "\n",
    "trained_vectors = pad_sequences(trained_vectors, padding='post', maxlen=maxlen)\n",
    "trained_answer = sample_train_data['emotion']\n",
    "target = pad_sequences(target, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(trained_vectors[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a9918ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(trained_vectors, trained_answer, test_size=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a835a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309367, 100)\n",
      "(309367,)\n",
      "(9569, 100)\n",
      "(9569,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be005ce9",
   "metadata": {},
   "source": [
    "## Text vectorization with w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "842f3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_features = [word_tokenize(each_train_text) for each_train_text in sample_train_data['text']]\n",
    "tokenized_test_features = [word_tokenize(each_test_text) for each_test_text in test_data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ea41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(\n",
    "    tokenized_train_features,\n",
    "    vector_size=vector_size,\n",
    "    window=20,\n",
    "    min_count=1,\n",
    "    sg=1  # 1 for skip-gram; otherwise CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ee0f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LH',\n",
       " '>',\n",
       " '<',\n",
       " '#',\n",
       " '@',\n",
       " '.',\n",
       " '!',\n",
       " 'the',\n",
       " 'I',\n",
       " 'to',\n",
       " ',',\n",
       " 'a',\n",
       " 'and',\n",
       " 'you',\n",
       " 'is',\n",
       " '?',\n",
       " 'of',\n",
       " 'for',\n",
       " 'in',\n",
       " 'my',\n",
       " 'on',\n",
       " 'it',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"n't\",\n",
       " 'me',\n",
       " 'your',\n",
       " 'are',\n",
       " 'be',\n",
       " '...',\n",
       " 'with',\n",
       " 'have',\n",
       " 'this',\n",
       " '&',\n",
       " '’',\n",
       " 'not',\n",
       " 'at',\n",
       " 'do',\n",
       " 'so',\n",
       " 'all',\n",
       " 'just',\n",
       " 'was',\n",
       " 'but',\n",
       " 'The',\n",
       " \"'m\",\n",
       " 'up',\n",
       " 'like',\n",
       " '``',\n",
       " \"''\",\n",
       " 'can',\n",
       " ':',\n",
       " 'get',\n",
       " 'we',\n",
       " 'out',\n",
       " 'You',\n",
       " 'from',\n",
       " 'what',\n",
       " 'will',\n",
       " 'about',\n",
       " 'by',\n",
       " '=',\n",
       " 'when',\n",
       " 'they',\n",
       " 'today',\n",
       " 'life',\n",
       " 'one',\n",
       " '..',\n",
       " 'people',\n",
       " 'as',\n",
       " 'day',\n",
       " 'It',\n",
       " 'no',\n",
       " 'love',\n",
       " 'time',\n",
       " 'now',\n",
       " 'he',\n",
       " 'has',\n",
       " 'who',\n",
       " 'an',\n",
       " 'God',\n",
       " '-',\n",
       " 'know',\n",
       " 'or',\n",
       " 'i',\n",
       " 'only',\n",
       " 'how',\n",
       " 'more',\n",
       " ')',\n",
       " 'got',\n",
       " 'realDonaldTrump',\n",
       " 'am',\n",
       " 'our',\n",
       " 'if',\n",
       " 's',\n",
       " 'good',\n",
       " 'go',\n",
       " 'there',\n",
       " 't',\n",
       " 'make',\n",
       " 'see',\n",
       " 'did',\n",
       " 'his',\n",
       " 'What',\n",
       " 'been',\n",
       " \"'re\",\n",
       " 'So',\n",
       " '(',\n",
       " 'back',\n",
       " 'When',\n",
       " 'My',\n",
       " '2',\n",
       " 'going',\n",
       " 'want',\n",
       " 'u',\n",
       " 'still',\n",
       " '....',\n",
       " 'need',\n",
       " 'would',\n",
       " 'some',\n",
       " 'really',\n",
       " 'them',\n",
       " 'work',\n",
       " 'We',\n",
       " 'us',\n",
       " 'ca',\n",
       " 'new',\n",
       " 'had',\n",
       " 'much',\n",
       " 'their',\n",
       " ';',\n",
       " '2017',\n",
       " 'than',\n",
       " 'If',\n",
       " 'This',\n",
       " 'why',\n",
       " 'never',\n",
       " 'even',\n",
       " 'think',\n",
       " 'Just',\n",
       " 'her',\n",
       " 'does',\n",
       " 'over',\n",
       " \"'ve\",\n",
       " 'too',\n",
       " 'right',\n",
       " 'Why',\n",
       " 'being',\n",
       " 'off',\n",
       " 'come',\n",
       " 'A',\n",
       " 'That',\n",
       " 'say',\n",
       " 'And',\n",
       " 'way',\n",
       " 'because',\n",
       " 'last',\n",
       " 'him',\n",
       " 'He',\n",
       " 'she',\n",
       " 'always',\n",
       " 'feel',\n",
       " 'then',\n",
       " 'How',\n",
       " 'should',\n",
       " 'here',\n",
       " 'night',\n",
       " 'great',\n",
       " 'ever',\n",
       " 'again',\n",
       " 'Do',\n",
       " 'na',\n",
       " 'm',\n",
       " 'give',\n",
       " 'first',\n",
       " 'someone',\n",
       " 'No',\n",
       " 'after',\n",
       " 'Thank',\n",
       " '|',\n",
       " 'world',\n",
       " 'could',\n",
       " 'into',\n",
       " 'week',\n",
       " 'best',\n",
       " 'man',\n",
       " 'tonight',\n",
       " 'show',\n",
       " 'please',\n",
       " 'Life',\n",
       " 'things',\n",
       " '*',\n",
       " 'were',\n",
       " 'down',\n",
       " '1',\n",
       " 'play',\n",
       " \"'\",\n",
       " 'better',\n",
       " 'days',\n",
       " 'take',\n",
       " '3',\n",
       " 'thing',\n",
       " 'where',\n",
       " 'year',\n",
       " 'help',\n",
       " 'Is',\n",
       " 'every',\n",
       " 'once',\n",
       " \"'ll\",\n",
       " 'let',\n",
       " 'getting',\n",
       " 'many',\n",
       " 'something',\n",
       " 'watch',\n",
       " 'these',\n",
       " '4',\n",
       " 'But',\n",
       " 'other',\n",
       " 'Trump',\n",
       " '“',\n",
       " '$',\n",
       " 'shit',\n",
       " 'another',\n",
       " 'before',\n",
       " '”',\n",
       " 'made',\n",
       " 'morning',\n",
       " 'any',\n",
       " 'next',\n",
       " 'those',\n",
       " 'done',\n",
       " 'home',\n",
       " 'pips',\n",
       " 'game',\n",
       " 'very',\n",
       " 'watching',\n",
       " 'look',\n",
       " 'believe',\n",
       " 'said',\n",
       " 'follow',\n",
       " 'They',\n",
       " 'hope',\n",
       " 'wait',\n",
       " 'same',\n",
       " 'lol',\n",
       " 'everyone',\n",
       " 'keep',\n",
       " 'through',\n",
       " 'FifthHarmony',\n",
       " 'hate',\n",
       " 'Love',\n",
       " 'most',\n",
       " 'feeling',\n",
       " 'doing',\n",
       " 'Not',\n",
       " '😂',\n",
       " 'tomorrow',\n",
       " 'everything',\n",
       " 'moments',\n",
       " 'find',\n",
       " 'In',\n",
       " 'thanks',\n",
       " 'Issa',\n",
       " 'bad',\n",
       " 'real',\n",
       " 'season',\n",
       " 'away',\n",
       " 'nothing',\n",
       " 'years',\n",
       " 'person',\n",
       " 'its',\n",
       " 'happy',\n",
       " 'friends',\n",
       " 'Never',\n",
       " 'Jesus',\n",
       " 'live',\n",
       " 'All',\n",
       " 'lost',\n",
       " 'There',\n",
       " 'well',\n",
       " 'Lawrence',\n",
       " 'yet',\n",
       " 'family',\n",
       " 'sad',\n",
       " 'own',\n",
       " 'gon',\n",
       " 'dream',\n",
       " 'stop',\n",
       " 'long',\n",
       " 'without',\n",
       " 'already',\n",
       " 'around',\n",
       " 'Your',\n",
       " 'amazing',\n",
       " 'having',\n",
       " 'Thanks',\n",
       " 'while',\n",
       " 'Good',\n",
       " 'Hey',\n",
       " 'thought',\n",
       " 'Be',\n",
       " 'put',\n",
       " 'Let',\n",
       " 'ass',\n",
       " 'old',\n",
       " 'tell',\n",
       " 'Happy',\n",
       " \"'d\",\n",
       " 'hard',\n",
       " 'job',\n",
       " 'Now',\n",
       " 'guys',\n",
       " 'makes',\n",
       " 'Today',\n",
       " 'To',\n",
       " 'two',\n",
       " 'coming',\n",
       " 'trying',\n",
       " 'Can',\n",
       " '.....',\n",
       " 'call',\n",
       " 'phone',\n",
       " 'girl',\n",
       " 'Who',\n",
       " 'total',\n",
       " 'little',\n",
       " 'since',\n",
       " 'dreams',\n",
       " 'fucking',\n",
       " 'else',\n",
       " 'ur',\n",
       " 'POTUS',\n",
       " 'crazy',\n",
       " 'Oh',\n",
       " '5',\n",
       " 'For',\n",
       " 'money',\n",
       " 'anything',\n",
       " 'sure',\n",
       " 'actually',\n",
       " 'anyone',\n",
       " 'end',\n",
       " 'team',\n",
       " '🇨🇦CAD',\n",
       " 'YOU',\n",
       " '🕘',\n",
       " 'change',\n",
       " '🇬🇧GBP',\n",
       " '🇨🇳CNY',\n",
       " '🇷🇺RUB',\n",
       " 'such',\n",
       " '🇺🇸USD',\n",
       " '🇪🇺EUR',\n",
       " 'sleep',\n",
       " 'times',\n",
       " '0,16',\n",
       " 'Have',\n",
       " 'win',\n",
       " 'fuck',\n",
       " 'service',\n",
       " 'talk',\n",
       " 'U',\n",
       " 'use',\n",
       " 'don',\n",
       " 'until',\n",
       " 'wrong',\n",
       " 'must',\n",
       " 'yourself',\n",
       " 'looking',\n",
       " 'may',\n",
       " 'big',\n",
       " 'myself',\n",
       " 'hear',\n",
       " 'enough',\n",
       " 'episode',\n",
       " 'name',\n",
       " 'guy',\n",
       " 'weekend',\n",
       " 'making',\n",
       " 'school',\n",
       " 'left',\n",
       " 'Please',\n",
       " 'friend',\n",
       " 'Well',\n",
       " 'thank',\n",
       " 'Lord',\n",
       " 're',\n",
       " 'waiting',\n",
       " 'Lots',\n",
       " '10',\n",
       " 'also',\n",
       " 'She',\n",
       " 'fear',\n",
       " '%',\n",
       " 'Day',\n",
       " 'says',\n",
       " 'food',\n",
       " 'working',\n",
       " 'Wow',\n",
       " 'mind',\n",
       " 'girls',\n",
       " 'hours',\n",
       " 'beautiful',\n",
       " 'Ca',\n",
       " 'house',\n",
       " 'playing',\n",
       " 'whole',\n",
       " 'care',\n",
       " 'try',\n",
       " 'THE',\n",
       " 'lot',\n",
       " 'ready',\n",
       " 'which',\n",
       " 'One',\n",
       " '❤️',\n",
       " 'tweet',\n",
       " 'Closed',\n",
       " 'seen',\n",
       " 'EURUSD',\n",
       " 'place',\n",
       " 'mean',\n",
       " 'gt',\n",
       " 'WTF',\n",
       " 'needs',\n",
       " 'wish',\n",
       " 'wo',\n",
       " 'car',\n",
       " 'Twitter',\n",
       " 'His',\n",
       " 'excited',\n",
       " 'wan',\n",
       " 'face',\n",
       " 'August',\n",
       " 'music',\n",
       " 'song',\n",
       " 'November',\n",
       " 'went',\n",
       " 'full',\n",
       " 'faith',\n",
       " 'power',\n",
       " 'called',\n",
       " 'Are',\n",
       " 'kids',\n",
       " 'NOT',\n",
       " 'news',\n",
       " 'moment',\n",
       " 'true',\n",
       " 'Did',\n",
       " 'told',\n",
       " 'People',\n",
       " 'MostRequestLive',\n",
       " 'looks',\n",
       " '😭',\n",
       " 'free',\n",
       " 'blessed',\n",
       " 'OUT',\n",
       " 'women',\n",
       " 'stay',\n",
       " 'against',\n",
       " 'both',\n",
       " 'word',\n",
       " 'saw',\n",
       " '20',\n",
       " 'Me',\n",
       " 'n',\n",
       " 'soon',\n",
       " 'talking',\n",
       " 'though',\n",
       " 'hell',\n",
       " 'others',\n",
       " 'found',\n",
       " 'im',\n",
       " 'gets',\n",
       " 'Insecure',\n",
       " 'country',\n",
       " 'damn',\n",
       " 'head',\n",
       " 'words',\n",
       " 'OnAirRomeo',\n",
       " 'few',\n",
       " 'ago',\n",
       " 'America',\n",
       " 'ask',\n",
       " 'understand',\n",
       " 'reason',\n",
       " 'different',\n",
       " 'together',\n",
       " 'Great',\n",
       " 'miss',\n",
       " 'stupid',\n",
       " 'Molly',\n",
       " 'weeks',\n",
       " 'comes',\n",
       " 'IssaRae',\n",
       " 'Got',\n",
       " 'bed',\n",
       " 'birthday',\n",
       " 'run',\n",
       " 'weird',\n",
       " 'New',\n",
       " '🙄',\n",
       " 'Sunday',\n",
       " 'alone',\n",
       " 'came',\n",
       " '[',\n",
       " ']',\n",
       " 'nice',\n",
       " 'gone',\n",
       " 'saying',\n",
       " 'wants',\n",
       " 'hour',\n",
       " 'leave',\n",
       " 'thinking',\n",
       " 'baby',\n",
       " 'fun',\n",
       " '🤔',\n",
       " 'On',\n",
       " 'twitter',\n",
       " 'minutes',\n",
       " 'men',\n",
       " 'taking',\n",
       " 'become',\n",
       " 'ta',\n",
       " 'month',\n",
       " 'order',\n",
       " 'IS',\n",
       " 'r',\n",
       " 'October',\n",
       " 'CALLS',\n",
       " 'awesome',\n",
       " 'worst',\n",
       " 'eat',\n",
       " 'kind',\n",
       " 'followers',\n",
       " '6',\n",
       " 'ppl',\n",
       " 'each',\n",
       " 've',\n",
       " 'point',\n",
       " 'open',\n",
       " 'far',\n",
       " 'proud',\n",
       " 'pay',\n",
       " 'literally',\n",
       " 'thankful',\n",
       " 'remember',\n",
       " 'cause',\n",
       " 'might',\n",
       " 'turn',\n",
       " 'As',\n",
       " 'happen',\n",
       " 'Where',\n",
       " 'everyday',\n",
       " 'mad',\n",
       " 'read',\n",
       " 'anymore',\n",
       " 'future',\n",
       " 'finally',\n",
       " 'used',\n",
       " 'late',\n",
       " 'Still',\n",
       " 'white',\n",
       " 'class',\n",
       " 'Like',\n",
       " 'Friday',\n",
       " 'took',\n",
       " 'Of',\n",
       " 'woman',\n",
       " 'watched',\n",
       " 'wtf',\n",
       " 'Yes',\n",
       " 'happened',\n",
       " 'past',\n",
       " 'First',\n",
       " 'Some',\n",
       " 'trust',\n",
       " 'started',\n",
       " 'almost',\n",
       " 'CNN',\n",
       " 'tired',\n",
       " 'instead',\n",
       " 'Get',\n",
       " 'IT',\n",
       " 'sick',\n",
       " 'Really',\n",
       " 'special',\n",
       " 'Watching',\n",
       " 'goes',\n",
       " 'forward',\n",
       " '😍',\n",
       " 'Hope',\n",
       " 'half',\n",
       " 'least',\n",
       " 'buy',\n",
       " 'heard',\n",
       " 'seeing',\n",
       " 'wanted',\n",
       " 'story',\n",
       " 'living',\n",
       " 'Only',\n",
       " 'TO',\n",
       " 'black',\n",
       " '7',\n",
       " 'months',\n",
       " 'matter',\n",
       " 'pray',\n",
       " 'room',\n",
       " '😩',\n",
       " 'won',\n",
       " 'shame',\n",
       " 'mom',\n",
       " 'following',\n",
       " 'truly',\n",
       " '1st',\n",
       " 'movie',\n",
       " 'yesterday',\n",
       " 'video',\n",
       " 'dead',\n",
       " 'fan',\n",
       " 'GOP',\n",
       " '30',\n",
       " 'during',\n",
       " 'stuff',\n",
       " 'gave',\n",
       " 'idea',\n",
       " \"y'all\",\n",
       " 'customer',\n",
       " 'media',\n",
       " 'pretty',\n",
       " 'move',\n",
       " 'business',\n",
       " 'Another',\n",
       " 'lose',\n",
       " 'Christmas',\n",
       " 'less',\n",
       " 'able',\n",
       " 'Halloween',\n",
       " 'giving',\n",
       " 'means',\n",
       " 'party',\n",
       " 'book',\n",
       " 'tweets',\n",
       " 'Thanksgiving',\n",
       " 'under',\n",
       " '8',\n",
       " 'hit',\n",
       " 'single',\n",
       " 'high',\n",
       " 'vote',\n",
       " 'post',\n",
       " 'funny',\n",
       " 'stand',\n",
       " 'guess',\n",
       " 'Time',\n",
       " 'bring',\n",
       " 'perfect',\n",
       " 'dont',\n",
       " 'Sometimes',\n",
       " 'At',\n",
       " 'side',\n",
       " 'hair',\n",
       " 'NFL',\n",
       " 'w/',\n",
       " 'FoxNews',\n",
       " 'Here',\n",
       " 'pissed',\n",
       " 'set',\n",
       " 'walk',\n",
       " 'bit',\n",
       " 'Buy',\n",
       " '12',\n",
       " '😱',\n",
       " '~',\n",
       " 'shows',\n",
       " 'Monday',\n",
       " 'Nothing',\n",
       " 'Sad',\n",
       " 'AND',\n",
       " 'Christ',\n",
       " 'grateful',\n",
       " 'body',\n",
       " 'check',\n",
       " '😡',\n",
       " 'top',\n",
       " 'Him',\n",
       " 'knows',\n",
       " 'September',\n",
       " '1,42',\n",
       " 'Our',\n",
       " '😂😂😂',\n",
       " 'between',\n",
       " 'boy',\n",
       " 'ai',\n",
       " '😊',\n",
       " 'dog',\n",
       " 'early',\n",
       " 'account',\n",
       " 'forever',\n",
       " 'Hi',\n",
       " 'Im',\n",
       " '😂😂',\n",
       " 'fight',\n",
       " 'loved',\n",
       " 'knew',\n",
       " 'ya',\n",
       " 'Go',\n",
       " 'water',\n",
       " 'Feeling',\n",
       " 'success',\n",
       " '1.0',\n",
       " 'fake',\n",
       " 'answer',\n",
       " 'Keep',\n",
       " 'With',\n",
       " 'break',\n",
       " 'ALL',\n",
       " 'Every',\n",
       " 'die',\n",
       " 'ones',\n",
       " 'child',\n",
       " '—',\n",
       " 'ok',\n",
       " '11',\n",
       " 'seriously',\n",
       " '15',\n",
       " 'bitch',\n",
       " 'later',\n",
       " 'eyes',\n",
       " 'Lol',\n",
       " 'tho',\n",
       " '--',\n",
       " 'cool',\n",
       " 'Man',\n",
       " 'learn',\n",
       " 'asked',\n",
       " 'line',\n",
       " 'wonder',\n",
       " 'jealous',\n",
       " 'bc',\n",
       " 'rest',\n",
       " 'god',\n",
       " 'feels',\n",
       " 'forget',\n",
       " 'pain',\n",
       " 'Does',\n",
       " 'football',\n",
       " 'Was',\n",
       " 'send',\n",
       " 'seems',\n",
       " 'behind',\n",
       " 'using',\n",
       " 'd',\n",
       " 'dude',\n",
       " 'lt',\n",
       " 'TV',\n",
       " 'Damn',\n",
       " 'dad',\n",
       " 'anger',\n",
       " 'death',\n",
       " 'Always',\n",
       " 'fall',\n",
       " 'hot',\n",
       " 'safe',\n",
       " 'yes',\n",
       " 'favorite',\n",
       " 'thoughts',\n",
       " 'glad',\n",
       " 'lives',\n",
       " 'KILLED',\n",
       " 'meet',\n",
       " 'US',\n",
       " 'soul',\n",
       " 'OMG',\n",
       " 'problem',\n",
       " 'listen',\n",
       " 'health',\n",
       " 'DonnieWahlberg',\n",
       " 'Sell',\n",
       " 'Will',\n",
       " 'Its',\n",
       " 'hands',\n",
       " 'running',\n",
       " 'sorry',\n",
       " 'parents',\n",
       " 'brother',\n",
       " 'hand',\n",
       " 'listening',\n",
       " 'fans',\n",
       " 'son',\n",
       " 'MY',\n",
       " 'realize',\n",
       " 'control',\n",
       " 'Had',\n",
       " 'Aug',\n",
       " 'children',\n",
       " 'Even',\n",
       " 'super',\n",
       " 'Looking',\n",
       " 'Very',\n",
       " 'trump',\n",
       " 'Shame',\n",
       " 'hold',\n",
       " 'oh',\n",
       " 'USA',\n",
       " 'played',\n",
       " 'American',\n",
       " '......',\n",
       " 'truth',\n",
       " '😒',\n",
       " 'second',\n",
       " 'coffee',\n",
       " 'John',\n",
       " 'boys',\n",
       " 'light',\n",
       " 'online',\n",
       " 'angry',\n",
       " 'bought',\n",
       " 'December',\n",
       " 'speak',\n",
       " 'deal',\n",
       " 'strong',\n",
       " 'poor',\n",
       " '❤',\n",
       " 'President',\n",
       " '😰',\n",
       " 'till',\n",
       " 'Come',\n",
       " 'cold',\n",
       " 'date',\n",
       " 'Someone',\n",
       " 'fact',\n",
       " 'After',\n",
       " 'app',\n",
       " 'bless',\n",
       " 'wow',\n",
       " 'These',\n",
       " 'SO',\n",
       " '😯',\n",
       " 'July',\n",
       " 'experience',\n",
       " '😨',\n",
       " 'happening',\n",
       " 'wake',\n",
       " 'maybe',\n",
       " 'human',\n",
       " 'Paul',\n",
       " 'tickets',\n",
       " 'NO',\n",
       " 'annoyed',\n",
       " 'act',\n",
       " 'outside',\n",
       " 'close',\n",
       " 'either',\n",
       " 'paid',\n",
       " 'office',\n",
       " 'Everyone',\n",
       " 'social',\n",
       " 'luck',\n",
       " 'dinner',\n",
       " '+',\n",
       " 'MAGA',\n",
       " 'plan',\n",
       " 'worse',\n",
       " 'BB19',\n",
       " 'given',\n",
       " 'important',\n",
       " 'takes',\n",
       " 'voice',\n",
       " 'starting',\n",
       " 'president',\n",
       " 'small',\n",
       " 'greatest',\n",
       " 'peace',\n",
       " 'loves',\n",
       " 'Finally',\n",
       " 'shall',\n",
       " '9',\n",
       " 'drive',\n",
       " 'worth',\n",
       " 'wife',\n",
       " 'question',\n",
       " 'Anyone',\n",
       " 'Then',\n",
       " '🙏',\n",
       " 'needed',\n",
       " 'course',\n",
       " '/',\n",
       " 'cute',\n",
       " '100',\n",
       " 'OF',\n",
       " 'cant',\n",
       " 'w',\n",
       " 'gives',\n",
       " 'May',\n",
       " 'enjoy',\n",
       " 'turned',\n",
       " 'chance',\n",
       " 'Or',\n",
       " 'woke',\n",
       " 'keeps',\n",
       " 'YouTube',\n",
       " 'pussy',\n",
       " 'front',\n",
       " 'works',\n",
       " 'DM',\n",
       " 'didn',\n",
       " 'longer',\n",
       " 'share',\n",
       " 'Saturday',\n",
       " 'self',\n",
       " 'Stop',\n",
       " 'train',\n",
       " 'smh',\n",
       " 'Millions',\n",
       " 'eating',\n",
       " 'missed',\n",
       " 'Seriously',\n",
       " 'absolutely',\n",
       " 'probably',\n",
       " 'Also',\n",
       " 'fire',\n",
       " 'games',\n",
       " 'vs',\n",
       " 'Because',\n",
       " 'll',\n",
       " 'Omg',\n",
       " 'flight',\n",
       " 'sitting',\n",
       " 'Yeah',\n",
       " 'nigga',\n",
       " 'joke',\n",
       " 'entire',\n",
       " 'number',\n",
       " 'Big',\n",
       " 'insecure',\n",
       " 'ON',\n",
       " 'cut',\n",
       " 'mine',\n",
       " 'disappointed',\n",
       " 'Last',\n",
       " 'reading',\n",
       " 'sex',\n",
       " 'due',\n",
       " 'Ur',\n",
       " 'college',\n",
       " 'possible',\n",
       " 'x',\n",
       " 'definitely',\n",
       " 'dick',\n",
       " 'door',\n",
       " 'finished',\n",
       " 'IN',\n",
       " 'tv',\n",
       " 'ME',\n",
       " 'missing',\n",
       " 'card',\n",
       " 'drink',\n",
       " 'heart',\n",
       " 'writing',\n",
       " '😦',\n",
       " 'album',\n",
       " 'gym',\n",
       " 'tried',\n",
       " 'Lost',\n",
       " 'kill',\n",
       " 'Been',\n",
       " 'sound',\n",
       " 'wonderful',\n",
       " '40',\n",
       " 'couple',\n",
       " 'Everything',\n",
       " 'broken',\n",
       " 'rather',\n",
       " 'fail',\n",
       " 'store',\n",
       " 'fix',\n",
       " 'gift',\n",
       " 'respect',\n",
       " 'supposed',\n",
       " 'fast',\n",
       " 'lies',\n",
       " 'alive',\n",
       " 'World',\n",
       " 'Don',\n",
       " 'LOVE',\n",
       " 'Am',\n",
       " 'young',\n",
       " 'motivation',\n",
       " 'happens',\n",
       " '31',\n",
       " 'Dear',\n",
       " 'More',\n",
       " 'fine',\n",
       " 'Being',\n",
       " 'Make',\n",
       " 'catch',\n",
       " 'starts',\n",
       " 'Stay',\n",
       " 'positive',\n",
       " 'dumb',\n",
       " 'relationship',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(w2v_model.wv.key_to_index.keys())\n",
    "vocab_size = len(vocab_list)\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_OOV_vocab(sample: list, list_vocab):\n",
    "    \"\"\" Takes in tokenized sample in the form of list \n",
    "    and the vocabulary list and removes tokens from sample\n",
    "    that are not in the vocabulary list\"\"\"\n",
    "    in_vocab_sample = []\n",
    "    for each_token in sample:\n",
    "        if each_token in list_vocab:\n",
    "            in_vocab_sample.append(each_token)\n",
    "    return in_vocab_sample\n",
    "  \n",
    "tokenized_test_features = [remove_OOV_vocab(each_test_sample, vocab_list) for each_test_sample in tokenized_test_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1932cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v_model.wv.key_to_index.keys()\n",
    "embedding_matrix = w2v_model.wv[vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecce981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_indexed_token_sequences(w2v_model, list_features):\n",
    "    indexed_features = []\n",
    "    for each_seq in list_features:\n",
    "        list_token_indices = []\n",
    "        for each_token in each_seq:\n",
    "            try:\n",
    "                list_token_indices.append(w2v_model.wv.key_to_index[each_token])\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "        indexed_features.append(list_token_indices)\n",
    "    return indexed_features\n",
    "\n",
    "indexed_train_features = w2v_indexed_token_sequences(w2v_model, tokenized_train_features)\n",
    "indexed_test_features = w2v_indexed_token_sequences(w2v_model, tokenized_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ee31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 20\n",
    "\n",
    "padded_train = pad_sequences(indexed_train_features, padding = 'post', maxlen=max_seq_len, truncating='post')\n",
    "trained_answer = sample_train_data['emotion']\n",
    "padded_test = pad_sequences(indexed_test_features, padding = 'post', maxlen=max_seq_len, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "564d49ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padded_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-561ffbf1023e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrained_answer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.03\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'padded_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_train, trained_answer, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1bda5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 20)\n",
      "(180000,)\n",
      "(60000, 20)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e911e84",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78de4d",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f47be464",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "DT_model = DT_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_test_pred = DT_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d2b1d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.31\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "acc_train = accuracy_score(y_true=Y_test, y_pred=Y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0833391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2149  610  979  737  633  868  751  622]\n",
      " [ 579 2766  643  624  863  540  573  941]\n",
      " [1000  626 1955  746  587 1044  892  614]\n",
      " [ 737  646  747 2821  622  627  691  661]\n",
      " [ 648  809  643  680 2302  596  640 1096]\n",
      " [ 918  599 1132  714  651 2073  859  613]\n",
      " [ 831  606  973  773  741  864 2168  683]\n",
      " [ 658  940  679  625 1128  580  680 2204]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=Y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb625f0b",
   "metadata": {},
   "source": [
    "### naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "811b3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dddf9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_test_pred = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55b52397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.16\n"
     ]
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_true=Y_test, y_pred=Y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "600664bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1112  911  422 2511  407  364  891  731]\n",
      " [ 835 1492  391 2310  440  386  977  698]\n",
      " [ 835  827  466 2521  483  383 1190  759]\n",
      " [ 619  630  318 3309  382  337 1349  608]\n",
      " [ 717  690  376 2882  561  397 1169  622]\n",
      " [ 794  751  434 2713  400  418 1236  813]\n",
      " [ 742  695  394 2853  428  375 1467  685]\n",
      " [ 782  848  407 2772  417  396 1084  788]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=Y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83a31b",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "134ab423",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(n_estimators=100,n_jobs = -1,random_state =50, min_samples_leaf = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6153fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = forest_model.fit(X_train, Y_train)\n",
    "Y_test_pred = forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3a28b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_true=Y_test, y_pred=Y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd1605c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[258  80 207 134  54  84  49  45]\n",
      " [ 90 269 123 131  76  58  46  85]\n",
      " [151  91 216 145  44 104  48  58]\n",
      " [ 88  81 134 335  57  64  48  81]\n",
      " [101 114 129 188 202  64  50  85]\n",
      " [151  73 189 152  74 181  51  50]\n",
      " [104  69 170 187  72  87 150  62]\n",
      " [116  94 136 140  96  82  58 189]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=Y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ebfa3",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e0a5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dfbdeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "LR = LR.fit(X_train, Y_train)\n",
    "Y_test_pred = LR.predict(X_test)\n",
    "target_result = LR.predict(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ceae543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.15\n"
     ]
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_true=Y_test, y_pred=Y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd5d8737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[242  43 443  33   0  97  49   4]\n",
      " [181 120 413  25   0  85  50   4]\n",
      " [146  49 470  33   0 124  33   2]\n",
      " [157  27 401  68   0 148  84   3]\n",
      " [169  41 449  52   0 146  72   4]\n",
      " [154  45 495  36   0 129  61   1]\n",
      " [150  29 476  51   0 132  62   1]\n",
      " [174  47 474  41   0 113  62   0]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=Y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abef86",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff8f7b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "## Before conver\n",
      "\n",
      "521966         fear\n",
      "1061014    surprise\n",
      "186615        trust\n",
      "924065         fear\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "\n",
      "## After convert\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_data['emotion'])\n",
    "print(\"classes:\", label_encoder.classes_)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "print('## Before conver\\n')\n",
    "print(Y_train[0:4])\n",
    "def label_encode(le, oe, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return oe.fit_transform(enc.reshape((len(enc), 1)))\n",
    "    #return enc\n",
    "    \n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "Y_train = label_encode(label_encoder, onehot_encoder, Y_train)\n",
    "Y_test = label_encode(label_encoder, onehot_encoder, Y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print(Y_train[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "340ec89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  100\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823ffc6",
   "metadata": {},
   "source": [
    "### deep learning model\n",
    "kera taught by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0202e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               64128     \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,672\n",
      "Trainable params: 81,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, GlobalAvgPool1D\n",
    "from keras.layers import ReLU, Softmax, Dropout\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=128)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=128)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a4b565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<180000x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2376028 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d23a9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change to sparse tensor, so it can be model's input\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))\n",
    "\n",
    "X_train_enc = convert_sparse_matrix_to_sparse_tensor(X_train)\n",
    "X_test_enc = convert_sparse_matrix_to_sparse_tensor(X_test)\n",
    "target_enc = convert_sparse_matrix_to_sparse_tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39027c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x2cd8a777250>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b288237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.7189 - accuracy: 0.3551 - val_loss: 1.6604 - val_accuracy: 0.3820\n",
      "Epoch 2/50\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6142 - accuracy: 0.3995 - val_loss: 1.6281 - val_accuracy: 0.3941\n",
      "Epoch 3/50\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5714 - accuracy: 0.4175 - val_loss: 1.6289 - val_accuracy: 0.3947\n",
      "Epoch 4/50\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5380 - accuracy: 0.4307 - val_loss: 1.6325 - val_accuracy: 0.3913\n",
      "Epoch 5/50\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5091 - accuracy: 0.4426 - val_loss: 1.6348 - val_accuracy: 0.3935\n",
      "Epoch 6/50\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4830 - accuracy: 0.4535 - val_loss: 1.6475 - val_accuracy: 0.3901\n",
      "Epoch 7/50\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4601 - accuracy: 0.4621 - val_loss: 1.6537 - val_accuracy: 0.3897\n",
      "Epoch 8/50\n",
      "1046/1407 [=====================>........] - ETA: 1s - loss: 1.4305 - accuracy: 0.4737"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-00c9a9e6240f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# training!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m history = model.fit(X_train_enc, Y_train, \n\u001b[0m\u001b[0;32m     11\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/kaggle_training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train_enc, Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test_enc, Y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67cdfb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3219/3219 [==============================] - 6s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'anticipation', 'disgust', 'trust', 'trust'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predict\n",
    "target_result = model.predict(target_enc, batch_size=128)\n",
    "target_result = label_decode(label_encoder, target_result)\n",
    "target_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46f57a",
   "metadata": {},
   "source": [
    "### keras with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf348122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x15d25fc7400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae164a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 100, 50)           500000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 100, 80)          29120     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 80)               38720     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 648       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 568,488\n",
      "Trainable params: 568,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=10000, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.Bidirectional(layers.LSTM(40, return_sequences=True)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(40)))\n",
    "#model.add(layers.Dense(2048, activation='relu'))\n",
    "#model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87a12833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30937/30937 [==============================] - 500s 16ms/step - loss: 1.5124 - accuracy: 0.4424 - val_loss: 1.4306 - val_accuracy: 0.4730\n",
      "Epoch 2/20\n",
      "30937/30937 [==============================] - 501s 16ms/step - loss: 1.3444 - accuracy: 0.5096 - val_loss: 1.3876 - val_accuracy: 0.4895\n",
      "Epoch 3/20\n",
      " 8494/30937 [=======>......................] - ETA: 5:57 - loss: 1.2480 - accuracy: 0.5492"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1a9bde58d9ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(X_train, Y_train,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     batch_size=10)\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1570\u001b[1;33m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \"\"\"\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"end\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             raise ValueError(\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1158\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m         \u001b[1;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# as-is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1155\u001b[0m     \"\"\"\n\u001b[0;32m   1156\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1158\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1123\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, Y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1412ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_result = model.predict(target)\n",
    "target_result = label_decode(label_encoder, target_result)\n",
    "target_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289297d1",
   "metadata": {},
   "source": [
    "# result to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ec684",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv = pd.DataFrame(columns=['id', 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv['id'] = test_data['tweet_id']\n",
    "result_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv['emotion'] = target_result\n",
    "result_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ee5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv.to_csv(\"kaggle_data/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0dd57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
